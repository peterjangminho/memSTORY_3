package com.memstory.llm

import ai.onnxruntime.*
import ai.onnxruntime.providers.NNAPIFlags
import android.content.Context
import android.content.res.AssetManager
import android.util.Log
import com.google.gson.Gson
import com.google.gson.JsonObject
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.io.File
import java.io.FileOutputStream
import java.io.InputStream
import java.nio.IntBuffer

/**
 * ONNX Runtime-based LLM inference engine for Gemma 3 1B model
 * Handles model loading, tokenization, and text generation
 */
class OnnxLLMEngine(private val context: Context) {
    
    private var ortSession: OrtSession? = null
    private var ortEnvironment: OrtEnvironment? = null
    private lateinit var tokenizer: GemmaTokenizer
    private lateinit var modelConfig: ModelConfig
    
    companion object {
        private const val TAG = "OnnxLLMEngine"
        private const val MODEL_PATH = "models/gemma-3-1b"
        private const val MODEL_FILE = "model_q4f16.onnx"
        private const val CONFIG_FILE = "config.json"
        private const val TOKENIZER_FILE = "tokenizer.json"
        private const val GENERATION_CONFIG_FILE = "generation_config.json"
    }
    
    data class ModelConfig(
        val vocab_size: Int,
        val max_position_embeddings: Int,
        val hidden_size: Int,
        val num_attention_heads: Int,
        val num_hidden_layers: Int,
        val bos_token_id: Int,
        val eos_token_id: List<Int>,
        val sliding_window: Int,
        val cache_implementation: String,
        val num_key_value_heads: Int,
        val head_dim: Int,
        val use_cache: Boolean
    )
    
    data class GenerationConfig(
        val max_length: Int = 512,
        val temperature: Float = 0.7f,
        val top_p: Float = 0.9f,
        val do_sample: Boolean = true
    )
    
    /**
     * Initialize the LLM engine with model loading and tokenizer setup
     */
    suspend fun initialize(): Boolean = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "Initializing ONNX LLM Engine...")
            
            // Initialize ONNX Runtime Environment
            ortEnvironment = OrtEnvironment.getEnvironment()
            
            // Load model configuration
            modelConfig = loadModelConfig()
            Log.d(TAG, "Model config loaded: vocab_size=${modelConfig.vocab_size}")
            
            // Load tokenizer
            tokenizer = GemmaTokenizer.fromAssets(context.assets, "$MODEL_PATH/$TOKENIZER_FILE")
            Log.d(TAG, "Tokenizer loaded with vocab size: ${tokenizer.vocabSize}")
            
            // Load ONNX model with NNAPI acceleration for Snapdragon 8 Elite
            val modelFilePath = ensureModelInInternalStorage()
            
            // Create session options with NNAPI provider for Snapdragon 8 Elite NPU
            val sessionOptions = OrtSession.SessionOptions()
            try {
                // Configure NNAPI execution provider for NPU acceleration (1.22.0+ API)
                val nnapiFlags = java.util.EnumSet.of(
                    NNAPIFlags.USE_FP16  // Only use FP16 optimization, allow CPU cooperation
                )
                sessionOptions.addNnapi(nnapiFlags)
                sessionOptions.setOptimizationLevel(OrtSession.SessionOptions.OptLevel.ALL_OPT)
                
                ortSession = ortEnvironment?.createSession(modelFilePath, sessionOptions)
                Log.d(TAG, "ONNX model loaded with NNAPI execution provider (NPU acceleration)")
            } catch (e: Exception) {
                Log.w(TAG, "NNAPI not available, falling back to CPU only: ${e.message}")
                val fallbackOptions = OrtSession.SessionOptions()
                fallbackOptions.setOptimizationLevel(OrtSession.SessionOptions.OptLevel.ALL_OPT)
                ortSession = ortEnvironment?.createSession(modelFilePath, fallbackOptions)
                Log.d(TAG, "ONNX model loaded with CPU execution provider only")
            }
            
            Log.d(TAG, "LLM Engine initialization completed")
            true
        } catch (e: Exception) {
            Log.e(TAG, "Failed to initialize LLM Engine", e)
            false
        }
    }
    
    /**
     * Generate text response from user input
     */
    suspend fun generateResponse(userInput: String, maxTokens: Int = 256): String = withContext(Dispatchers.IO) {
        try {
            Log.d(TAG, "Generating response for: $userInput")
            
            // Tokenize input
            val inputTokens = tokenizer.encode(userInput)
            Log.d(TAG, "Input tokenized: ${inputTokens.size} tokens")
            
            Log.d(TAG, "Starting autoregressive generation with NNAPI acceleration")
            
            // Generate tokens using proper autoregressive generation with NNAPI
            val generatedTokens = generateTokens(inputTokens, maxTokens)
            
            // Decode response (skip input tokens)
            val response = tokenizer.decode(generatedTokens.drop(inputTokens.size))
            // Memory usage summary for this generation
            val finalMemoryInfo = """
                Generation completed:
                - Input tokens: ${inputTokens.size}
                - Generated tokens: ${generatedTokens.size - inputTokens.size}
                - Total conversation tokens: ${generatedTokens.size}
                - KV cache size: ${estimateKVCacheMemory(generatedTokens.size)}MB
            """.trimIndent()
            
            Log.d(TAG, finalMemoryInfo)
            Log.d(TAG, "Response generated: $response")
            
            // Cleanup handled in generateTokens() function
            
            response
        } catch (e: Exception) {
            Log.e(TAG, "Failed to generate response", e)
            "Sorry, I encountered an error while processing your request."
        }
    }
    
    /**
     * Gemma 3 1B optimized generation with sliding window (official config)
     * NNAPI + NPU acceleration with hybrid cache implementation
     */
    private suspend fun generateTokens(
        initialTokens: List<Int>,
        maxTokens: Int,
        temperature: Float = 0.7f,
        topP: Float = 0.9f
    ): List<Int> = withContext(Dispatchers.IO) {
        val generatedTokens = initialTokens.toMutableList()
        
        Log.d(TAG, "Gemma 3 1B generation: sliding_window=${modelConfig.sliding_window}, cache=${modelConfig.cache_implementation}")
        
        // Initialize KV cache for all 26 layers (Gemma 3 1B)
        // First inference: past_sequence_length = 0 (empty cache)
        var kvCache = mutableMapOf<String, OnnxTensor>()
        for (layer in 0 until modelConfig.num_hidden_layers) {
            // Initialize with empty cache: (batch=1, heads=1, seq_len=0, head_dim=256)
            val emptyKey = Array(1) { Array(modelConfig.num_key_value_heads) { Array(0) { FloatArray(modelConfig.head_dim) } } }
            val emptyValue = Array(1) { Array(modelConfig.num_key_value_heads) { Array(0) { FloatArray(modelConfig.head_dim) } } }
            
            kvCache["past_key_values.${layer}.key"] = OnnxTensor.createTensor(ortEnvironment, emptyKey)
            kvCache["past_key_values.${layer}.value"] = OnnxTensor.createTensor(ortEnvironment, emptyValue)
        }
        
        for (i in 0 until maxTokens) {
            try {
                // Apply sliding window (official Gemma 3 1B optimization)
                val windowedTokens = if (generatedTokens.size > modelConfig.sliding_window) {
                    generatedTokens.takeLast(modelConfig.sliding_window)
                } else {
                    generatedTokens
                }
                
                val inputIds = LongArray(windowedTokens.size) { windowedTokens[it].toLong() }
                val positionIds = LongArray(windowedTokens.size) { 
                    // Position IDs account for full sequence position
                    (generatedTokens.size - windowedTokens.size + it).toLong()
                }
                
                // Create tensors
                val inputTensor = OnnxTensor.createTensor(ortEnvironment, Array(1) { inputIds })
                val positionTensor = OnnxTensor.createTensor(ortEnvironment, Array(1) { positionIds })
                
                // Inputs following official config with KV cache
                val inputs = mutableMapOf<String, OnnxTensor>(
                    "input_ids" to inputTensor,
                    "position_ids" to positionTensor
                )
                inputs.putAll(kvCache)
                
                // Run ONNX inference with NNAPI (hybrid cache handled by ONNX Runtime)
                val outputs = ortSession?.run(inputs)
                val logits = outputs?.get(0)?.value as Array<Array<FloatArray>>
                
                // Get next token logits
                val nextTokenLogits = logits[0].last().copyOf()
                
                // Apply temperature
                for (j in nextTokenLogits.indices) {
                    nextTokenLogits[j] = nextTokenLogits[j] / temperature
                }
                
                // Sample next token
                val nextToken = sampleTopP(nextTokenLogits, topP)
                
                // Check for EOS
                if (modelConfig.eos_token_id.contains(nextToken)) {
                    Log.d(TAG, "EOS token reached at position ${generatedTokens.size}")
                    break
                }
                
                generatedTokens.add(nextToken)
                
                // Update KV cache with new outputs (present.X.key â†’ past_key_values.X.key)
                val newKvCache = mutableMapOf<String, OnnxTensor>()
                for (layer in 0 until modelConfig.num_hidden_layers) {
                    // Outputs: [logits, present.0.key, present.0.value, present.1.key, present.1.value, ...]
                    val keyOutput = outputs?.get(1 + layer * 2) as OnnxTensor
                    val valueOutput = outputs?.get(2 + layer * 2) as OnnxTensor
                    
                    newKvCache["past_key_values.${layer}.key"] = keyOutput
                    newKvCache["past_key_values.${layer}.value"] = valueOutput
                }
                
                // Close old KV cache tensors to prevent memory leak
                kvCache.values.forEach { it.close() }
                kvCache = newKvCache
                
                // Cleanup immediately (memory efficient)
                inputTensor.close()
                positionTensor.close()
                
                // Log sliding window efficiency
                if ((generatedTokens.size % 100) == 0) {
                    val windowSize = minOf(generatedTokens.size, modelConfig.sliding_window)
                    Log.d(TAG, "Generated ${generatedTokens.size} tokens (window: $windowSize)")
                }
                
            } catch (e: Exception) {
                Log.e(TAG, "Error generating token $i: ${e.message}")
                break
            }
        }
        
        // Final cleanup of KV cache
        kvCache.values.forEach { it.close() }
        
        generatedTokens
    }
    
    /**
     * Estimate KV cache memory usage for conversation length
     * Gemma 3 1B: 26 layers Ã— 1 head Ã— tokens Ã— 256 dim Ã— 2 bytes Ã— 2 (K+V)
     */
    private fun estimateKVCacheMemory(numTokens: Int): Float {
        // Gemma 3 1B GQA configuration
        val numLayers = modelConfig.num_hidden_layers // 26
        val numKeyValueHeads = 1 // GQA optimization
        val headDim = 256 // modelConfig.head_dim
        val bytesPerFloat16 = 2
        
        // KV cache = Key + Value tensors
        val kvCacheBytes = numLayers * numKeyValueHeads * numTokens * headDim * bytesPerFloat16 * 2
        
        return kvCacheBytes.toFloat() / (1024 * 1024) // Convert to MB
    }
    
    /**
     * Top-p (nucleus) sampling implementation
     */
    private fun sampleTopP(logits: FloatArray, topP: Float): Int {
        // Convert logits to probabilities using softmax
        val maxLogit = logits.maxOrNull() ?: 0f
        val expLogits = logits.map { kotlin.math.exp((it - maxLogit).toDouble()).toFloat() }
        val sumExp = expLogits.sum()
        val probabilities = expLogits.map { it / sumExp }
        
        // Create probability-index pairs and sort by probability descending
        val sortedProbs = probabilities.mapIndexed { index, prob -> Pair(prob, index) }
            .sortedByDescending { it.first }
        
        // Calculate cumulative probabilities
        var cumSum = 0f
        val selectedIndices = mutableListOf<Int>()
        
        for ((prob, index) in sortedProbs) {
            cumSum += prob
            selectedIndices.add(index)
            if (cumSum >= topP) break
        }
        
        // Sample from selected indices
        val randomValue = kotlin.random.Random.nextFloat() * cumSum
        var currentSum = 0f
        
        for (index in selectedIndices) {
            currentSum += probabilities[index]
            if (randomValue <= currentSum) {
                return index
            }
        }
        
        return selectedIndices.last()
    }
    
    /**
     * Load model configuration from assets
     */
    private fun loadModelConfig(): ModelConfig {
        val configStream = context.assets.open("$MODEL_PATH/$CONFIG_FILE")
        val configJson = configStream.bufferedReader().use { it.readText() }
        val gson = Gson()
        val jsonObject = gson.fromJson(configJson, JsonObject::class.java)
        
        return ModelConfig(
            vocab_size = jsonObject.get("vocab_size").asInt,
            max_position_embeddings = jsonObject.get("max_position_embeddings").asInt,
            hidden_size = jsonObject.get("hidden_size").asInt,
            num_attention_heads = jsonObject.get("num_attention_heads").asInt,
            num_hidden_layers = jsonObject.get("num_hidden_layers").asInt,
            bos_token_id = jsonObject.get("bos_token_id").asInt,
            eos_token_id = gson.fromJson(jsonObject.get("eos_token_id"), List::class.java).map { (it as Double).toInt() },
            sliding_window = jsonObject.get("sliding_window")?.asInt ?: 512,
            cache_implementation = jsonObject.get("cache_implementation")?.asString ?: "hybrid",
            num_key_value_heads = jsonObject.get("num_key_value_heads")?.asInt ?: 1,
            head_dim = jsonObject.get("head_dim")?.asInt ?: 256,
            use_cache = jsonObject.get("use_cache")?.asBoolean ?: true
        )
    }
    
    /**
     * Ensure model is in internal storage for native heap loading
     * Java heap: ~1MB for configs, Native heap: ~998MB for model weights
     */
    private suspend fun ensureModelInInternalStorage(): String = withContext(Dispatchers.IO) {
        val modelsDir = File(context.filesDir, "models/gemma-3-1b")
        if (!modelsDir.exists()) {
            modelsDir.mkdirs()
        }
        
        val modelFile = File(modelsDir, MODEL_FILE)
        
        // Check if model exists and is valid (998MB expected)
        if (!modelFile.exists() || modelFile.length() < 990_000_000L) {
            Log.d(TAG, "Copying 998MB model from assets to internal storage...")
            Log.d(TAG, "Memory strategy: Java heap (configs) + Native heap (model weights + KV cache)")
            
            copyAssetToInternalStorage("$MODEL_PATH/$MODEL_FILE", modelFile)
            
            Log.d(TAG, "Model ready for native heap loading: ${modelFile.length()} bytes")
        } else {
            Log.d(TAG, "Model already in internal storage: ${modelFile.length()} bytes")
        }
        
        modelFile.absolutePath
    }
    
    /**
     * Stream-based asset copying (avoids Java heap OOM)
     */
    private fun copyAssetToInternalStorage(assetPath: String, destinationFile: File) {
        context.assets.open(assetPath).use { inputStream ->
            FileOutputStream(destinationFile).use { outputStream ->
                val buffer = ByteArray(64 * 1024) // 64KB streaming buffer
                var totalBytes = 0L
                var bytesRead: Int
                
                while (inputStream.read(buffer).also { bytesRead = it } != -1) {
                    outputStream.write(buffer, 0, bytesRead)
                    totalBytes += bytesRead
                    
                    // Log progress every 100MB
                    if (totalBytes % (100 * 1024 * 1024) == 0L) {
                        Log.d(TAG, "Copied ${totalBytes / (1024 * 1024)}MB...")
                    }
                }
                outputStream.flush()
                Log.d(TAG, "Total copied: ${totalBytes / (1024 * 1024)}MB")
            }
        }
    }
    
    /**
     * Get comprehensive memory usage information
     */
    fun getMemoryInfo(): String {
        val modelSizeMB = 998 // model_q4f16.onnx size
        val tokenizerSizeMB = 33 // tokenizer.json size
        val javaHeapMB = 51 // Estimated Java heap usage
        
        return """
            MemSTORY LLM Engine Memory Usage:
            
            ðŸ“‹ JAVA HEAP (Safe Zone):
            - Model configs: ~1MB
            - Tokenizer configs: ~1MB  
            - App overhead: ~49MB
            - Total Java heap: ${javaHeapMB}MB / 512MB limit
            
            ðŸ§  NATIVE HEAP (ONNX Runtime Managed):
            - Model weights (mmap): ${modelSizeMB}MB virtual (~200-300MB physical)
            - Tokenizer vocab: ${tokenizerSizeMB}MB
            - KV cache (varies): 0-52MB (depends on conversation length)
            - Tensor buffers: ~10MB
            - Total native heap: ~295-395MB
            
            ðŸ’¡ GQA OPTIMIZATION:
            - Standard attention: 4 heads Ã— 52MB = 208MB KV cache
            - Gemma 3 1B GQA: 1 head Ã— 52MB = 52MB KV cache  
            - Memory saving: 75% reduction!
            
            ðŸŽ¯ TOTAL MEMORY: ~346-446MB (8GB device = Very Safe)
        """.trimIndent()
    }
    
    /**
     * Clean up all resources (Java + Native heaps)
     */
    fun cleanup() {
        try {
            Log.d(TAG, "Cleaning up LLM Engine resources...")
            
            // Cleanup ONNX Runtime (native heap)
            ortSession?.close()
            ortEnvironment?.close()
            
            // Cleanup tokenizer (native heap)
            tokenizer.cleanup()
            
            Log.d(TAG, "All LLM Engine resources cleaned up")
            Log.d(TAG, "Memory freed: ~395MB native heap + ~51MB Java heap")
            
        } catch (e: Exception) {
            Log.e(TAG, "Error during cleanup", e)
        }
    }
}